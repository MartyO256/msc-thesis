\chapter{Discussion and Conclusion}\label{chapter:conclusion}

The objectives of this thesis were to improve the design and implementation of \Beluga and \Harpoon in order fix soundness issues with the incremental development of proofs in interactive \ac{REPL} sessions.
This required revising the first few phases of syntactic and semantic analysis for the language in order to make them modular and reusable with respect to different instances of processing states.

The grammar for \Beluga and \Harpoon was fully formalized in \ac{EBNF}, along with a specification of syntactic ambiguities and how they are resolved using precedence levels and grammar blurring.
The parser was improved with the introduction of a new context-sensitive disambiguation phase.
This prevented the propagation of ambiguous \ac{AST} nodes into the more complex phases of \Beluga's processing pipeline.
By the same token, the language's was modified to support non-normal terms, which in turn allowed for syntax error messages to be improved.
The user-defined notation feature, previously limited to terms in the index language, is now fully supported at the computation-level as well.
These changes make the implementation robust, and the language more expressive and cohesive, which in turn improves the user experience.

The indexing phase was fully reimplemented using a new structure of the referencing environment at all \ac{AST} nodes in \Beluga programs and \Harpoon proof scripts.
This structure simplified the name resolution algorithm and allowed for the unification of constant declarations and indexing contexts to support simple lexical scoping rules.
In tandem, the existing implementation of namespaces for grouping signature-level declarations was rectified.
De Bruijn indices computations were implemented, and indexing for the \LF part of \Beluga was formalized to prove that the stack structure of frames can be safely leveraged to implement a checkpoint mechanism for the bindings in scope.
During \Harpoon structural editing sessions on proof scripts, the soundness of navigating between holes with respect to name resolution was rectified by ensuring that disambiguation and indexing are explicitly dependent on the referencing environment's state.

The following sections conclude this work with an evaluation of the implemented changes to \Beluga and \Harpoon, a discussion of future work to rectify long-standing issues in the later phases of semantic analysis, and finally closing remarks on the design and implementation of the system.

\section{Evaluation}


\subsection*{Runtime Performance}

% Why was performance a concern?

Runtime performance is one of the main concerns for \Beluga and \Harpoon.
Indeed, because dependently-typed programming languages are more difficult to work with than typical languages, assisting the user with data generated during syntactic and semantic analyses is instrumental to program development.
For example, one of \Beluga's most used feature is that of displaying type information about holes in programs
Users tend to run type-checking at every step of program development to constantly refresh that information.
Consequently, in the implementation of \Beluga, a greater emphasis was placed on early error detection and reporting to minimize the latency between signature type-checking requests and the display of useful information.

% What were the impacts of the changes on performance?

Signature reconstruction has been sufficiently fast for most of the mechanizations realised in \Beluga~\texttt{v1.0}.
Consequently, the new parsing and indexing phases introduced in \texttt{v1.1} had to not degrade the runtime performance of the system.
This objective was surpassed: on a set of \num{429} test \Beluga signatures totalling approximately \num{35000} lines of code, signature reconstruction ran on average in \SI{8.412(0.133)}{\second} in \texttt{v1.0} and \SI{8.045(0.126)}{\second} in \texttt{v1.1}, which means there was a \SI{4}{\percent} runtime improvement with the revised parsing and indexing phases.
To ensure a fair comparison, these test cases were selected such that they had at most minor differences between them to mitigate the impact of breaking changes to the syntax.
The system was compiled using the same version of its dependencies, and the average of \num{15} runs was used with \num{3} warmup runs.

With an identical methodology, a similar test was carried out on a larger mechanization: with an average signature reconstruction runtime of \SI{5.457(0.013)}{\second} in \texttt{v1.0} and \SI{3.956(0.011)}{\second} in \texttt{v1.1}, there was a \SI{3}{\percent} runtime improvement.
This mechanization is mostly comprised of verbose \Harpoon proof scripts tallying over \num{90000} lines of code, which provides a better estimate for the time required to completely parse, reconstruct and type-check larger projects.

Although there were changes to \Beluga between \texttt{v1.0} and \texttt{v1.1} outside of the parsing, disambiguation and indexing phases, the performance improvement is best explained by the simplification of the grammar for parsing, and the usage of mutable dictionaries instead of association lists for name resolution.
Additionally, having a uniform referencing environment structure reduced the number of lookup misses that would occur when sequentially looking up declaration tables.

\subsection*{Maintainability}

% Why is maintainability a concern?

The \Beluga system has accrued significant technical debt throughout its development cycles.
This is partly explained by turnover-induced knowledge loss and lack of unit tests.
Initiatives were put in place to tackle these issues in \texttt{v1.1} during the revision of the parser.

% What were the impacts of the changes on maintainability?

To avoid turnover-induced knowledge loss, the specification for \Beluga's and \Harpoon's grammar from appendix~\ref{chapter:grammar} has been added to the code repository.
Additionally, the grammar manipulations and disambiguation mechanisms required for parsing is documented more thoroughly in the implementation.
This ensures that those steps can be repeated in the future, should the grammar be amended to support new features.

The parser implemented in \texttt{v1.0} returned \acp{AST} containing ambiguous nodes, and would postpone the handling of user-defined operators to the indexing phase.
This discouraged the implementation of unit tests for the parser since its outputs were incomplete and more likely to change.
In contrast, the parser revised in \texttt{v1.1} returns \acp{AST} in an unambiguous external syntax representation.
Because of this, \num{345} unit tests were added to verify the parser in isolation from the rest of the system.
These tests check success and failure scenarios for parsing small inputs for the important syntactic categories of the language and with respect to mock disambiguation states.
A unit-testing approach this precise could not have been reliably implemented in \texttt{v1.0}.
Additionally, partial round-trip testing of the parser was added for existing \Beluga examples and case studies using a pretty-printer for signatures in external syntax representation.
Complete round-trip testing is not supported since the parser discards inline comments during lexical analysis.


% hyperfine --warmup 3 --runs 15 "opam exec ./TEST"
% hyperfine --warmup 3 --runs 15 "opam exec ./TEST.sh"

%i9-12900k v1.0
%Benchmark 1: opam exec ./TEST
%  Time (mean ± σ):      8.412 s ±  0.133 s    [User: 6.677 s, System: 1.642 s]
%  Range (min … max):    8.139 s …  8.640 s    15 runs

%i9-12900k v1.1
%Benchmark 1: opam exec ./TEST.sh
%  Time (mean ± σ):      8.045 s ±  0.126 s    [User: 6.106 s, System: 1.839 s]
%  Range (min … max):    7.821 s …  8.256 s    15 runs

%i5-1135G7 v1.0
%Benchmark 1: opam exec ./TEST
%  Time (mean ± σ):      8.863 s ±  0.089 s    [User: 7.508 s, System: 1.239 s]
%  Range (min … max):    8.766 s …  9.087 s    15 runs

%i5-1135G7 v1.1
%Benchmark 1: opam exec ./TEST.sh
%  Time (mean ± σ):      8.343 s ±  0.087 s    [User: 6.573 s, System: 1.657 s]
%  Range (min … max):    8.165 s …  8.459 s    15 runs

%i7-3770k v1.0
%Benchmark 1: opam exec ./TEST
%  Time (mean ± σ):      16.237 s ±  0.041 s    [User: 14.828 s, System: 1.481 s]
%  Range (min … max):    16.177 s …  16.321 s    15 runs

%i7-3770k v1.1
%Benchmark 1: opam exec ./TEST.sh
%  Time (mean ± σ):      14.830 s ±  0.041 s    [User: 13.025 s, System: 1.879 s]
%  Range (min … max):    14.776 s …  14.894 s    15 runs

\clearpage
\begin{figure}[H]
\centering
\makebox[\textwidth][c]{
\begin{tabular}{lccc}
System & \href{https://github.com/Beluga-lang/Beluga/releases/tag/v1.0}{\Beluga \texttt{v1.0}} runtime (\si{\second}) & \href{https://github.com/Beluga-lang/Beluga/releases/tag/v1.1}{\Beluga \texttt{v1.1}} runtime (\si{\second}) & Difference (\si{\percent})\\
System 1\footnotemark & \SI{8.412(0.133)}{} & \SI{8.045(0.126)}{} & \SI{-4.36(2.18)}{}\\
System 2\footnotemark & \SI{8.863(0.089)}{} & \SI{8.343(0.087)}{} & \SI{-5.87(1.41)}{}\\
System 3\footnotemark & \SI{16.237(0.041)}{} & \SI{14.830(0.041)}{} & \SI{-8.67(0.36)}{}
\end{tabular}
}
\caption[Runtime performance improvement from \Beluga \texttt{v1.0} to \texttt{v1.1}]{%
Runtime performance improvement from \Beluga \texttt{v1.0} to \texttt{v1.1}.
These runtimes are the average of 15 runs, with 3 warmup runs.
}
\label{figure:runtime-improvement}
\end{figure}
\footnotetext[1]{Run on an Intel\textsuperscript{\textregistered} Core\textsuperscript{\texttrademark} i9-12900K processor}
\footnotetext[2]{Run on an Intel\textsuperscript{\textregistered} Core\textsuperscript{\texttrademark} i5-1135G7 processor}
\footnotetext[3]{Run on an Intel\textsuperscript{\textregistered} Core\textsuperscript{\texttrademark} i7-3770K processor}

\section{Future Work}

With regards to the implementation of \Beluga and \Harpoon, properly defining referencing environments and implementing indexing with respect to them is a step in the right direction.
Indeed, this allows the indexing procedures to be sound and reusable when visiting holes in \Harpoon proofs in an out-of-order fashion with respect to the \Beluga signature.
This also enables indexing to be unit-tested independently of the processing pipeline, which offers greatly visibility into the inner workings of the system, as well as provides more fine-grained verifiers of the implementation's correctness.

There are nonetheless many implementation challenges left ahead in order to fully support structural editing of \Beluga programs and \Harpoon proofs.
Indeed, there are areas of the system which are still reliant on the assumption that signatures are always processed in order of declaration, and hence that the signature reconstruction store only contains data about visible declarations.
The future areas of work on the implementation of \Beluga and \Harpoon include the following:

\begin{enumerate}
\item
Information flow analysis is required in the later phases of semantic analysis, namely type and term reconstruction, type-checking and unification, to ascertain whether their stateful operations are always handled soundly.
Specifically, while there is a trailing mechanism for higher-order unification to keep track of meta-variable instantiations (the assignment of a contextual object to a meta-variable), there are routines during \LF reconstruction that ignore this bookkeeping.
This can result in unsound programs when users undo edit actions during interactive proof developments.
Fixing this issue will require significant refactoring to decouple those phases from global data structures such as the signature reconstruction store.
\item
The logic proof search engine which powers \Harpoon's automation tactics uses the signature reconstruction store to have a global view of the user-defined constants and programs that can be used to solve subgoals.
As such, synthesized solutions to subgoals may reference declarations that are out of scope.
That notwithstanding, taking all constants into account during proof search may result in degraded performance and a non-responsive \ac{REPL} when using automation tactics on larger projects.
Enforcing the constraint that synthesized proofs must be sound with respect to name resolution at the holes they have to be spliced in may prune the search tree.
\item
Fresh name generation for the nameless representation of \Beluga programs is currently unsound in the implementation.
Indeed, that procedure does not wholly take into account the identifiers in scope as it restricts its focus on the declarations in indexing contexts.
This has consequences with program synthesis, both for the conversion of \Harpoon proof scripts to \Beluga programs and for error reporting.
Much like was the case with the now resolved soundness issue of navigating to holes in \Harpoon proofs, spliced \Beluga programs corresponding to \Harpoon proof scripts may refer to inadvertedly shadowed constants.
That is, a generated variable name, which is assumed to be fresh, may actually clash with a required constant name.
To address this, one needs to synthesize programs in a nameless representation and then compute readable and easily distinguishable names for variables, which effectively amounts to reversing indexing.
Dynamic programming over the tree structure of the \ac{AST} would be required to compute sets of referenced identifiers and de Bruijn indices to select appropriate names at the binding sites.
\end{enumerate}

\section{Final Remarks}

The key lessons learned from this experience with regards to programming language design and implementation are as follows:

\begin{enumerate}
\item
If a programming language is context-sensitive, separate its syntactic analysis into a context-free and a context-sensitive phase, if possible.
This ensures good runtime performance for the parser, and facilitates the integration of the language with tools that primarily support context-free languages.
\item
Keep the syntax accepted during context-free parsing simple, and rely on subsequent processing phases to disambiguate complex syntax overloading and enforce additional syntactic restrictions.
This enables error messages to be augmented with some semantic analysis to suggest corrections.
\item
Keep disambiguation and name resolution mechanisms simple and intuitive.
This ensures that user programs will be readable, at least at the syntactic level.
\item
Ensure that the programming language's grammar supports desugared forms for all syntactic sugars, as well as syntax for explicitly supplying what are otherwise implicit terms.
This facilitates the traceability of programs from their parsed representation to their elaborated representation, it allows users to opt out of using syntactic sugars, and it enables users to observe and verify the results of term and type reconstruction.
\item
Avoid the common programming pitfall of relying on global mutable data to capture the implemented system's state.
There will always be execution scenarios incompatible with that design, in particular incremental development, unit testing and concurrency.
\end{enumerate}
