\chapter{Discussion and Conclusion}

\section{Evaluation}

As established in chapters~\ref{chapter:parsing-reimplementation} and \ref{chapter:indexing-reimplementation}, the parsing and indexing phases of \Beluga had to be reimplemented between versions \texttt{1.0} and \texttt{v1.1} to rectify soundness issues in incremental proof development.
This functional requirement is verified by way of unit tests for the parser and integration tests for the indexing phase as part of testing for the entire processing pipeline.
Non-functional requirements such as maintainability and testability were also improved for those phases.
Indeed, proven design patterns were introduced in the codebase to facilitate both the development of new features and the rectification of parts of \Beluga's existing architecture.
Stateful operations during parsing, disambiguation and indexing are strictly local to those phases, and decoupled from the rest of the system.
Despite the introduced changes being justified, they are not without drawbacks.

Breaking changes to the \Beluga language had to be introduced both at the level of its syntax and its semantics.
Enforcing unambiguous syntaxes for meta-level objects and types, as well as removing support for overloaded identifiers made the system more robust, but also made it less flexible.
Additionally, the introduction of the disambiguation phase separate from the indexing phase involved duplicating the name resolution algorithm.
Duplicated routines also appear in pretty-printing modules to format \Beluga signatures and print them as source code and \HTML pages.
On one hand, this code duplication ensures that those modules are independent of one another, but on the other hand, it complicates the process of changing \Beluga's concrete syntax.

While \Beluga versions \texttt{1.0} and \texttt{v1.1} are not functionally equivalent in some aspects, version \texttt{v1.1} brought significant performance improvements as evidence by figure~\ref{figure:runtime-improvement}.
These benchmarks are the result of running the entire processing pipeline on a set of examples testable in both versions, namely the \Beluga signatures in the \texttt{examples} directory, with minor adjustments limited to addressing the syntactic changes to meta-objects and coinductive observation applications.
The project was compiled using the same version of its dependencies in all cases to ensure fair comparisons.
Though there were changes to \Beluga between versions \texttt{1.0} and \texttt{v1.1} outside of the parsing, disambiguation and indexing phases, the performance improvement is best explained by the simplification of the grammar for parsing, and the usage of mutable dictionaries instead of association lists for name resolution.

% hyperfine --warmup 3 --runs 15 "opam exec ./TEST"
% hyperfine --warmup 3 --runs 15 "opam exec ./TEST.sh"

%i9-12900k v1.0
%Benchmark 1: opam exec ./TEST
%  Time (mean ± σ):      8.412 s ±  0.133 s    [User: 6.677 s, System: 1.642 s]
%  Range (min … max):    8.139 s …  8.640 s    15 runs

%i9-12900k v1.1
%Benchmark 1: opam exec ./TEST.sh
%  Time (mean ± σ):      8.045 s ±  0.126 s    [User: 6.106 s, System: 1.839 s]
%  Range (min … max):    7.821 s …  8.256 s    15 runs

%i5-1135G7 v1.0
%Benchmark 1: opam exec ./TEST
%  Time (mean ± σ):      8.863 s ±  0.089 s    [User: 7.508 s, System: 1.239 s]
%  Range (min … max):    8.766 s …  9.087 s    15 runs

%i5-1135G7 v1.1
%Benchmark 1: opam exec ./TEST.sh
%  Time (mean ± σ):      8.343 s ±  0.087 s    [User: 6.573 s, System: 1.657 s]
%  Range (min … max):    8.165 s …  8.459 s    15 runs

%i7-3770k v1.0
%Benchmark 1: opam exec ./TEST
%  Time (mean ± σ):      16.237 s ±  0.041 s    [User: 14.828 s, System: 1.481 s]
%  Range (min … max):    16.177 s …  16.321 s    15 runs

%i7-3770k v1.1
%Benchmark 1: opam exec ./TEST.sh
%  Time (mean ± σ):      14.830 s ±  0.041 s    [User: 13.025 s, System: 1.879 s]
%  Range (min … max):    14.776 s …  14.894 s    15 runs

\clearpage
\begin{figure}[H]
\centering
\makebox[\textwidth][c]{
\begin{tabular}{lccc}
System & \href{https://github.com/Beluga-lang/Beluga/releases/tag/v1.0}{\Beluga \texttt{v1.0}} runtime (\si{\second}) & \href{https://github.com/Beluga-lang/Beluga/releases/tag/v1.1}{\Beluga \texttt{v1.1}} runtime (\si{\second}) & Difference (\si{\percent})\\
System 1\footnotemark & \SI{8.412(0.133)}{} & \SI{8.045(0.126)}{} & \SI{-4.36(2.18)}{}\\
System 2\footnotemark & \SI{8.863(0.089)}{} & \SI{8.343(0.087)}{} & \SI{-5.87(1.41)}{}\\
System 3\footnotemark & \SI{16.237(0.041)}{} & \SI{14.830(0.041)}{} & \SI{-8.67(0.36)}{}
\end{tabular}
}
\caption[Runtime performance improvement from \Beluga \texttt{v1.0} to \texttt{v1.1}]{%
Runtime performance improvement from \Beluga \texttt{v1.0} to \texttt{v1.1}.
These runtimes are the average of 15 runs, with 3 warmup runs.
}
\label{figure:runtime-improvement}
\end{figure}
\footnotetext[1]{Run on an Intel\textsuperscript{\textregistered} Core\textsuperscript{\texttrademark} i9-12900K processor}
\footnotetext[2]{Run on an Intel\textsuperscript{\textregistered} Core\textsuperscript{\texttrademark} i5-1135G7 processor}
\footnotetext[3]{Run on an Intel\textsuperscript{\textregistered} Core\textsuperscript{\texttrademark} i7-3770K processor}

\section{Future Work}

Properly defining referencing environments and implementing indexing with respect to them is a step in the right direction.
Indeed, this opens up the indexing procedures to be sound and reusable when visiting holes in \Harpoon proofs in an out-of-order fashion with respect to the \Beluga signature.
This also enables indexing to be unit-tested independently of the processing pipeline, which offers greatly visibility into the inner workings of the system, as well as provides more fine-grained verifiers of the implementation's correctness.

There are nonetheless many implementation challenges left ahead in order to fully support structural editing of \Beluga programs and \Harpoon proofs.
Indeed, there are areas of the system which are still reliant on the assumption that signatures are always processed in order of declaration, and hence that the signature reconstruction store only contains data about visible declarations.

\begin{enumerate}
\item
Information flow analysis is required in the later phases of semantic analysis, namely type and term reconstruction, type-checking and unification, to ascertain whether their stateful operations are always handled soundly.
Specifically, while there is a trailing mechanism for higher-order unification to keep track of meta-variable instantiations (the assignment of a contextual object to a meta-variable), there are routines during \LF reconstruction that ignore this bookkeeping.
This can result in unsound programs when users undo edit actions during interactive proof developments.
Fixing this issue will require significant refactoring to decouple those phases from global data structures such as the signature reconstruction store.
\item
The logic proof search engine which powers \Harpoon's automation tactics uses the signature reconstruction store to have a global view of the user-defined constants and programs that can be used to solve subgoals.
As such, synthesized solutions to subgoals may reference declarations that are out of scope.
That notwithstanding, taking all constants into account during proof search may result in degraded performance and a non-responsive \ac{REPL} when using automation tactics on larger projects.
Enforcing the constraint that synthesized proofs must be sound with respect to name resolution at the holes they have to be spliced in may prune the search tree.
\item
Fresh name generation for the nameless representation of \Beluga programs is currently unsound in the implementation.
Indeed, that procedure does not wholly take into account the identifiers in scope as it restricts its focus on the declarations in indexing contexts.
This has consequences with program synthesis, both for the conversion of \Harpoon proof scripts to \Beluga programs and for error-reporting.
Much like was the case with the now resolved soundness issue of navigating to holes in \Harpoon proofs, spliced \Beluga programs corresponding to \Harpoon proof scripts may refer to shadowed constants.
To address this, one needs to synthesize programs in a nameless representation and then compute readable and easily distinguishable names for variables, which effectively amounts to reversing indexing.
Dynamic programming over the tree structure of the \ac{AST} would be required to compute sets of used identifiers and de Bruijn indices to select appropriate names at the binding sites.
\end{enumerate}

\section{Final Remarks}

In conclusion, \Beluga's parsing and indexing phases were reimplemented to address correctness, maintainability and soundness issues.
The language's grammar was formalized, then a two-phase syntactic analysis was implemented to first parse \Beluga signatures following a context-free grammar with overloaded syntactic constructs, and then disentangle those syntactic constructs in a context-sensitive disambiguation phase.
Name resolution for the language was reimplemented to unify constant declarations and indexing contexts into one referencing environment to support simple lexical scoping rules.
De Bruijn indices computations were implemented with respect to this referencing environment structure, and indexing for the \LF part of \Beluga was formalized.
During \Harpoon structural editing sessions on proof scripts, the soundness of navigating between holes with respect to name resolution was rectified by ensuring that disambiguation and indexing are explicitly dependant on the state of the referencing environment.
These improvements pave the way for the later phases of \Beluga's processing pipeline to be rectified.
The key lessons learned from this experience with regards to programming language design and implementation are as follows:

\begin{enumerate}
\item
If a programming language is context-sensitive, separate its syntactic analysis into a context-free and a context-sensitive phase, if possible.
This ensures good runtime performance for the parser, and facilitates the integration of the language with tools that primarily support context-free languages.
\item
Keep the syntax accepted during context-free parsing simple, and rely on subsequent processing phases to disambiguate complex syntax overloading and enforce additional syntactic restrictions.
This enables error messages to be augmented with some semantic analysis to suggest corrections.
\item
Keep disambiguation and name resolution mechanisms simple and intuitive.
This ensures that user programs will be readable, at least at the syntactic level.
\item
Ensure that the programming language's grammar supports desugared forms for all syntactic sugars, as well as syntax for explicitly supplying what are otherwise implicit terms.
This facilitates the traceability of programs from their parsed representation to their elaborated representation, it allows users to opt out of using syntactic sugars, and it enables users to observe and verify the results of term and type reconstruction.
\item
Avoid the common programming pitfall of relying on global mutable data to capture the implemented system's state.
There will always be execution scenarios incompatible with that design, in particular incremental development and unit testing.
\end{enumerate}
